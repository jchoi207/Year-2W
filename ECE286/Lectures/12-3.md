# Friday, April 5 Week 12-3


## Hypothesis Testing for one variance
**Hypothesis**:
- Null hypothesis $H_0: \sigma^2 = \sigma_0^2$
- Alternative hypothesis (two-sided): $H_1: \sigma^2 \neq \sigma_0^2$
- Alternative hypothesis (one-sided): $H_1: \sigma^2 > \sigma_0^2$ or $H_1: \sigma^2 < \sigma_0^2$ (we can look at right or left tail)

**Test Statistic**: 
$$\chi^2 = \frac{(n-1)S^2}{\sigma_0^2}$$
- Note that we use $v=n-1$ degrees of freedom

- Note that the chi squared distribution is not symmetric, so for a two-sided test we want to find both $\chi^2_{\alpha/2,v }$ and $\chi^2_{1- \alpha/2, v}$,

- Right tail: $\chi^2_{\alpha,v}$ 
- Left tail: $\chi^2_{1-\alpha,v}$


### Example 1
- $\sigma = 0.9$, $n=10$, $s = 1.2$
- Using a 0.05 level of significant, do you think that $\sigma > 0.9$ 
**Solution** 
- We are working with a right-tailed test: 
$$\chi^2 = \frac{(10-1)1.2^2}{0.9^2} = 16 $$ 
- We can use the table to find the chi-squared value $\chi^2_{0.05, 9} = 16.919$

Comparing statistics
- So since 16 < 16.919, we fail to reject the null hypothesis. Since our sample falls out of the critical region. 

Comparing p-values to critical
- Note we could also find the p-value: $P(\chi^2 > 16) = 0.07$, so since $ p > \alpha$ we fail to reject the null hypothesis.


## Hypothesis Testing for two variances
**Hypothesis**:
- Null hypothesis: $H_0: \sigma_1^2 = \sigma_2^2$
- Alternative hypothesis (two-sided): $H_1: \sigma_1^2 \neq \sigma_2^2$
- Alternative hypotehsis (one-sided): $H_1: \sigma_1^2 > \sigma_2^2$ or $H_1: \sigma_1^2 < \sigma_2^2$

**Test Statistic**:
$$ f = \frac{s_1^2}{s_2^2}$$
- Where $v_1 = n_1 - 1$ and $v_2 = n_2 - 1 $ 
- Note that the shape of the f-distribution is similar to the chi-squared
- *Note in the table, for each significance level, we have a new table* 

Reciprocal Relationship in the F-distribution: 
- In the table, there are only values for $\alpha = 0.05, 0.01$, so to find the left tail in a one or two sided test, we need to make use of the reciprocal relationship.
- Writing $f_{\alpha} (v_1 ,v_2)$ for $f$ with $v_1$ and $v_2$ dof: 
$$f_{1-\alpha} (v_1, v_2) = \frac{1}{f_{\alpha}(v_2, v_1)}$$ 

### Example Using the Reciprocal Relationship
- Let's say we have $\alpha = 0.1$, $v_1 = 5$ and $v_2 = 8$, so we need to find both $f_{\alpha/2}(5,9)$ and $f_{1-\alpha/2}(5,9)$, but in the table, we don't have that second value, so we know that $f_{1-\alpha/2}(5,9) = \frac{1}{f_{\alpha/2}(9,5)} = \frac{1}{4.82}$


### Example 2
- In the previous example we assumed that the two unknown
- $S_1 ^2 = 16$, $n_1 = 12$, $s_2 ^2 = 24 $ and $n_2 = 10$




## Goodness of Fit
Setup: 
- Discrete random variable (RV) with possible outcomes $i = 1,...,k$ 
- $n$ trials with expected frequencies $e_1 = nP(i)$ for outcomes $i=   1,...,k$
- Observed frequencies $o_i$ for outcomes $i$. $O_i$ is the RV
- Imagine that we roll a die 600 times, the expected frequency for getting any one of the numbers is 100. Out of those 600, we are going to count how many 1s, 2s, ... 6s we realized, so $o_1, o_2, ... o_6$ are the observed frequencies and $e_1, e_2, ... e_6$ are the expected frequencies.
- We define the test Statistic as: 
$$\chi^2 = \sum_{i=1}^{k} \frac{(o_i - e_i)^2}{e_i}$$

- We have a chi squared distribution with $v = k-1$ dog. 
- A small $\chi^2$ value indicates a good fit between the observed and expected frequencies 
- The â€¢critical region* is to the right tail of the chi-squared distribution for a *significance level $\alpha$*. 
Then $\chi^2 > \chi^2_{\alpha}$ is the critical region for rejection


### Example 3
- $chi^2 = 1.7$ 
- Choose an $\alpha$ and then find $\chi^2_{\alpha}$, and then compare the two, if $\chi^2 > \chi^2_{\alpha}$, then we reject the null hypothesis.