
# Week 10-2 Thursday March 21

## Interval Estimation of Variance

-   **Estimator of Variance:** if a sample size $n$ is drawn from a
    normal population with variance $\sigma^2$ and the sample variance
    $s^2$ is computed (realization), we obtain **a** value of the
    statistic $S^2$. This computed sample variance is used as a point
    estimate of $\sigma^2$. Hence the statistic $S^2$ is an estimator of
    $\sigma^2$

-   Interval estimate of $\sigma^2$ can be established using the
    statistic $$\chi^2 = X^2 = \frac{(n-1)S^2}{\sigma^2}$$ which has a
    chi-squared distribution with $n-1$ degrees of freedom when samples
    are chosen from a normal population

    -   Note, $\chi^2$ must can take on values from $[0, \infty)$

    -   

## Confidence Interval for Variance

-   The confidence interval for $\sigma^2$ is written as
    $$P(X^2_{\alpha/2} < X^2 < X^2_{1-\alpha/2}) = 1- \alpha = P\left(\frac{(n-1)S^2}{\chi^2_{\alpha/2}} \leq \sigma^2 \leq \frac{(n-1)S^2}{\chi^2_{1-\alpha/2}}\right)$$

-   Where $X^2_{1-\alpha/2}$ and $X^2_{\alpha/2}$ are the values of teh
    chi squared distribution with $n-1$ degrees of freedom, leaving
    areas $1- \alpha/2$ and $\alpha/2$ **to the right**

-   Note, the chi-squared distribution is not symmetric like the z and t
    distributions.

-   Note, to find the area, subtract the chi-squared value of the
    smaller one.

### Example

-   $n=10$, $s^2 = 0.286$

-   **Solution:** $\alpha/2 = 0.025$, find $\chi^2$ values with 9
    degrees of freedom
    $\chi^2_{0.025, 9} = 2.70 \ \chi^2_{0.975,9}= 19.023$

-   $\sigma^2 \in (0.135, 0.953)$

## Maximum Likelihood Estimation (MLE)

-   For some context, so far we've used intuitive statistics for
    parameter estimation

    -   Sample mean, $\bar{X}$ as an estimator for the population mean
        $\mu$

    -   Sample variance, $S^2$ as an estimator for the population
        variance $\sigma^2$

    -   Sample proportion $\hat{P} = X/n$ for the probability of
        sucesses $p$ in binomial trials

-   **Maximum likelihood estimation** is used for more complex models.
    MLE provides a method for estimating parameters by maximizing the
    likelihood function.

## Liklelihood Function

-   Given a sample $x_1, ..., x_n$ and a joint PDF
    $f(x_1, ..., x_n; \theta)$. The likelihood is defined as
    $$L(x_1, ...,x_n; \theta) = \prod_{i=1}^{n} g(x_i; \theta)$$

-   The MLE of $\theta$ is given by
    $\hat{\theta} = \text{arg max}_{\theta} = L(x_1, x...,x_n; \theta)$

-   Note, you need to know the distributions $g(x_i, \theta)$ to find a
    likelihood function

-   Note, to find the MLE, you take the derivative of $L$ and set it
    equal to zero. To check for minima/maxima, find the concavity.

### Example

-   Sample from a Bernoulli trial $\{1,0,1,1\}$, we aim to find the
    probability of a sucess $\theta = p$

-   **Solution:** Bernoulli trials with either fail or success is
    Binomial. Take the product of the bernoulli trials, prob of 3
    sucesses and 1 failure, is$$L(1,0,1,1;p) = {4 \choose 3} p^3 (1-p)$$

-   We differentiate w.r.t p, and we set the derivative = 0:
    ${4 \choose 3} (3\hat{p}^2 - 4\hat{p}^3) = 0 \rightarrow 3- 4\hat{p} = 0 \rightarrow \hat{p} = 3/4$