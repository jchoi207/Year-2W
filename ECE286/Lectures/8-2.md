# Thursday March 7

-   Consider observed data values $x_1, x_2, ... x_n$

-   Assume that they are realization of IID random variables
    $X_1, X_2, ... ,X_n$

-   **Sample mean as a point estimate**

    -   The sample statistic for mean is $\bar{X}$

    -   The observed sample mean is $\bar{x}$

    -   And $\mu$ represents the true but unknown population mean

-   **General notation for point estimates**

    -   $\theta$ denotes the **true parameter of the population**, same
        thing as $\mu$

    -   $\hat{\theta}$ is the point estimate observed from the sample,
        it is the actual realization of, akin to $\bar{x}$

    -   $\hat{\Theta}$ is the statistical estimator, **and thus is a
        distribution** similar to $\bar{X}$

-   Note that point estimates like the sample mean are used to infer
    properties of the population from the sample data

\
**Unbiased Estimator**

-   An estimator $\hat{\Theta}$ is an **unbiased** for a parameter
    $\theta$ is the expected value of $\hat{\Theta}$ is equal to
    $\theta$ $$E[\hat{\Theta}] = \theta$$

-   Example, the sample variance (when dividing by $n-1$), is an
    unbiased estimator of the population variance $\sigma^2$

-   **is $\mu$ an unbiased estimator**

-   yes because we can show that $E[\bar{X}] = \mu$ , as
    $\hat{\Theta} = \bar{X}$

-   $E[\bar{X}] = \frac{1}{n} \sum E[X_i] = \frac{1}{n} (n \mu)$, note
    this is because $E[X_i] = \mu$

-   **Note that any $X_i$ is also an unbiased estimator** since the
    expectation of any $X_i$ is equal to the population mean $\mu$ by
    definition $E[X_i] = \mu$

    -   Note, that this will likely not be the best estimator

\
**Efficient Estimators**

-   Among all the unbiased estimators of a parameter $\theta$, the one
    with the **smallest variance** is considered the most **efficient**
    estimator of $\theta$.

why do we use the sample mean $\bar{X}$ instead of any $X_i$

-   Look at variance of both:

    -   $var(X_i) = \sigma^2$

    -   $var(\bar{X}) = \frac{\sigma^2}{n}$

-   We see that the standard deviation of $\bar{X}$ is lower than that
    of $X_i$, thus, it is a more efficient estimator than a random i-th
    sample. The CLT informs us that the variance of the estimator
    $\bar{X}$ decreases at a rate of $n$, as $n$ becomes larger.

\
**Interval Estimation**

-   A point estimate $\hat{\theta}$ is a single best guess at the true
    parameter value $\theta$ but it is rarely exact.

-   An interval estimate provides a range
    $\theta_L \leq \theta \leq \theta_U$, which likely contains $\theta$

-   Acknowledging uncertainty is critical in statistical estimation

-   Example: weather forecasts for temperature always have a high and
    low

\
**Confidence Intervals**

-   The lower and upper bounds of a confidence interval, $\Theta_L$ and
    $\Theta_U$ are based on sample data

-   We seek to express our certainty in the interval with a statement
    like: $$P(\Theta_L \leq \theta \leq \Theta_U) = 1 - \alpha$$

-   Here $\alpha$ is a level of uncertainty that we are willing to
    accept

-   For example, if $\alpha = 0.05$, we obtain a $95\%$ confidence
    interval

    -   Intuition: the probability inside $\Theta_L$ and $\Theta_U$ is
        equal to 0.95

    -   Intuition: there is a 95% chance that the **sample parameter**
        is within our range

\
**Confidence Intervals for the Mean**

-   Sample mean $\bar{X}$

-   Variance of the sample mean from CLT:
    $\sigma_{\bar{X}}^2 = \frac{\sigma^2}{n}$

-   Statistic, $Z$ following a standard normal distribution if $\bar{X}$
    are normal or $n$ is large

-   for $\beta < 0.5$, $z_\beta$ satisfies $P( z < z_{\beta}) = \beta$

-   find a $z_\beta$ such that $\phi(-z_\beta) = \beta$ i.e.
    $z_\beta = - \phi^{-1}(\beta)$

-   **Confidence interval**

    -   $$P(-z_{\alpha/2} \leq Z \leq z_{\alpha/2}) = P(-z_{\alpha/2} \leq \frac{\bar{X} - \mu}{\sigma/\sqrt{n}}\leq z_{\alpha/2}) = 1 - \alpha$$

    -   $$P(\bar{X} - z_{\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}} \leq \mu \leq \bar{X} + z_{\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}})$$

### Example

-   Given $n = 20$

-   observed sample mean is $\bar{x} = 4$

-   the population standard deviation is known $\sigma = 2$

-   we aim to find the $95\%$ confidence interval

$$P(\mu_L < \mu < \mu_U) = 0.95$$
$$P(-z_{\alpha/2} < z < z_{\alpha/2}) = 0.95$$ The lower limit should be
$P(\frac{\mu_L-\bar{X}}{\sigma/\sqrt{n}} < \frac{\mu-\bar{X}}{\sigma/\sqrt{n}} < \frac{\mu_U-\bar{X}}{\sigma/\sqrt{n}})$

So $\mu_L = \bar{X}  - Z_{\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}}$ So
$\mu_U = \bar{X}  + Z_{\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}}$
$$z_{\alpha/2} = z_{(1-0.95)/2} = -\Phi^{-1}(0.025) \approx 1.96$$

### Example

-   $n=36$, $\bar{X} = 2.6$, $\sigma = 0.3$

-   Find the $95$ and $99 \%$ confidence intervals for the mean zinc
    concentration in the river.

-   $P(-z_{0.025} < z < z_{0.025}) = 0.95$

-   Therefore $-z_{0.025} = \frac{\mu_L - 2.6}{0.3/6}$ and
    $z_{0.025} = \frac{\mu_U - 2.6}{0.3/6}$ and thus
    $\mu_L, \mu_U = 2.6 \mp 1.96 (0.3/6)$

-   Similarly, for the $99\%$ confidence interval, we look for
    $\pm z_{0.005}$, which can be found by looking at
    $\Phi^{-1}(0.005) = z_{0.005} = -2.58$

-   Thus, $\mu_L, \mu_U = 2.6 \mp 2.58(0.3/6)$
