# Week 10-3 Friday March 22

## Log-Likelihood

-   When dealing wiht distributiosn that involve exponential functions,
    optimziing the likelihood cand be simplified by considering the
    logarithm of the likelihood (normal, Poisson, exponential
    distributions all involve exponentials)

-   Due to the non-lienarity of exponential functions, we apply
    logarithms. By taking the logarithm of the lilhood function, known
    as the log-likelihood simplifies the process:

    -   It reomves the exponential, resulting in a simplier function

    -   Since the logarithmic function is strictly increasing, the max
        of the likelihood is preserved

### Example: MLE for Normal Distrbiution

-   Given the following normal distrbiution, find the MLE:
    $$n(x;\mu , \sigma) = \frac{1}{\sqrt{2\pi}\sigma} e^{- \frac{1}{2}(\frac{x-\mu}{\sigma})^2}$$

-   **Solution:**

-   $$L(x_1, ..., x_n \sigma^2) = \prod_{i=1}^{n} \exp \left[-\frac{1}{2} (\frac{x_i - \mu}{\sigma})\right] = (2\pi \sigma^2)^{-n/2} \exp \left[- \frac{1}{2} \sum_{i=1}^n \left(\frac{x_i - \mu}{\sigma}\right)^2\right]$$
    $$= - \frac{n}{2}\ln(2\pi \sigma^2) - \frac{1}{2} \sum_{i=1}^{n} \left(\frac{x_i - \mu}{\sigma^2}\right)$$

-   Now we can take the derivativce w.r.t $\mu$ or $\sigma^2$
    $$\frac{\partial \ln(L)}{\partial \mu} = \sum_{i=1}^{n}(\frac{x_i - \mu}{\sigma}) = 0$$

-   We set the above equal to zero to find the estimator of $\hat{\mu}$
    we make $\mu \rightarrow \hat{\mu}$
    $$0 = \frac{1}{\sigma^2} \sum_{i=1}^{n} (x_i -\mu) \rightarrow \sum_{i=1}^n x_i - n \hat{\mu} \rightarrow \hat{\mu} = \frac{1}{n}\sum_{i=1}^{n} x_i = \bar{x}$$

-   Note, we take the same derivative w.r.t $\sigma^2$, the MLE of
    $\sigma^2$ is $\hat{\sigma}^2$ which has $n$ degrees of freedom, as
    such, it is a biased estimator of $\sigma^2$. Recall that the
    unbiased estimator $S^2$ had $n-1$ degrees of freedom.

### Example:

-   Given $12,11.2,13.5, 12.3, 13.8, 11.9$ comes fro ma population with
    density function $$f(x;\theta) = \begin{cases}
            \frac{\theta}{x^{\theta+1}}, \ \ x > 1, \\ 0, \ \ \text{else}
        \end{cases}$$

-   Where$\theta > 0$. Find the MLE of $\theta$
    $$L(x_1, ..., x_{6}; \theta) = \prod_{i=1}^{n} \frac{\theta}{x_i^{\theta+1}} = \frac{\theta^n}{(\prod_{i=1}^{n} x_i)^{\theta+1}}$$

-   $$= n \ln \theta  - (\theta +1 ) \ln \left(\prod_{i=1}^n x_i\right) = n \ln \theta - \sum_{i=1}^{6}(\theta + 1) \ln  x_i$$

-   now take the derivative of $\ln(L)$

-   $$\frac{\partial \ln(L)}{\partial \theta} = \frac{6}{\theta} - \sum_{i=1}^6 \ln(x_i) \rightarrow  0 = \frac{6}{\hat{\theta}} \sum_{i=1}^{6} \ln x_i \rightarrow \hat{\theta} = \frac{6}{\sum_{i=1}^6 x_i}$$

-   Note that it is easier to work with $x_i$ and then take the sum at
    the end.

-   Note, we only change the paramater to its estimator form
    $\theta \rightarrow \hat{\theta}$ when we set the derivative equal
    to 0.

-   $\hat{\theta} = 0.397$
